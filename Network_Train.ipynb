{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications import VGG19\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications import Xception\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import models\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xception (Model)             (None, 13, 13, 2048)      20861480  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 346112)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               177209856 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 198,074,927\n",
      "Trainable params: 177,213,447\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_conv_base = Xception(weights = 'imagenet', include_top = False, input_shape = (400,400,3))\n",
    "pretrained_conv_base.trainable = False\n",
    "model = models.Sequential()\n",
    "model.add(pretrained_conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation = 'relu', kernel_regularizer = regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizers.RMSprop(lr=1e-5), metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/Danny Han/Desktop/Plastic_Waste_Identifier_Project/Training_Image_DB'\n",
    "train_dir = base_dir\n",
    "val_dir = os.path.join(base_dir, \"validation\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3199 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "rotation_range=40,\n",
    "width_shift_range=0.2,\n",
    "height_shift_range=0.2,\n",
    "shear_range=0.2,\n",
    "zoom_range=0.2,\n",
    "horizontal_flip=True,\n",
    "fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_dir,\n",
    "target_size = (400, 400),\n",
    "batch_size = 10,\n",
    "class_mode = \"categorical\")\n",
    "\n",
    "#validation_generator = test_datagen.flow_from_directory(\n",
    "#val_dir,\n",
    "#target_size = (400, 400),\n",
    "#batch_size = 10,\n",
    "#class_mode = \"categorical\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\danny han\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\PIL\\Image.py:953: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 28s 470ms/step - loss: 2.7755 - acc: 0.3767\n",
      "Epoch 2/100\n",
      "60/60 [==============================] - 17s 276ms/step - loss: 2.3699 - acc: 0.4850\n",
      "Epoch 3/100\n",
      "60/60 [==============================] - 16s 264ms/step - loss: 2.1989 - acc: 0.5283\n",
      "Epoch 4/100\n",
      "60/60 [==============================] - 16s 265ms/step - loss: 2.1084 - acc: 0.5383\n",
      "Epoch 5/100\n",
      "60/60 [==============================] - 16s 264ms/step - loss: 2.0165 - acc: 0.5600\n",
      "Epoch 6/100\n",
      "60/60 [==============================] - 15s 243ms/step - loss: 1.8796 - acc: 0.5996\n",
      "Epoch 7/100\n",
      "60/60 [==============================] - 14s 235ms/step - loss: 1.7310 - acc: 0.6250\n",
      "Epoch 8/100\n",
      "60/60 [==============================] - 14s 233ms/step - loss: 1.7767 - acc: 0.5783\n",
      "Epoch 9/100\n",
      "60/60 [==============================] - 14s 239ms/step - loss: 1.7523 - acc: 0.6150\n",
      "Epoch 10/100\n",
      "60/60 [==============================] - 14s 240ms/step - loss: 1.7756 - acc: 0.6217\n",
      "Epoch 11/100\n",
      "60/60 [==============================] - 14s 235ms/step - loss: 1.6306 - acc: 0.6507\n",
      "Epoch 12/100\n",
      "60/60 [==============================] - 15s 243ms/step - loss: 1.6866 - acc: 0.5983\n",
      "Epoch 13/100\n",
      "60/60 [==============================] - 14s 233ms/step - loss: 1.5763 - acc: 0.6567\n",
      "Epoch 14/100\n",
      "60/60 [==============================] - 14s 238ms/step - loss: 1.5985 - acc: 0.6367\n",
      "Epoch 15/100\n",
      "60/60 [==============================] - 14s 233ms/step - loss: 1.5233 - acc: 0.6767\n",
      "Epoch 16/100\n",
      "60/60 [==============================] - 14s 240ms/step - loss: 1.6542 - acc: 0.6324\n",
      "Epoch 17/100\n",
      "60/60 [==============================] - 14s 237ms/step - loss: 1.5971 - acc: 0.6750\n",
      "Epoch 18/100\n",
      "60/60 [==============================] - 15s 248ms/step - loss: 1.5968 - acc: 0.6733\n",
      "Epoch 19/100\n",
      "60/60 [==============================] - 14s 234ms/step - loss: 1.4876 - acc: 0.6900\n",
      "Epoch 20/100\n",
      "60/60 [==============================] - 14s 235ms/step - loss: 1.5881 - acc: 0.6650\n",
      "Epoch 21/100\n",
      "60/60 [==============================] - 14s 237ms/step - loss: 1.5748 - acc: 0.6683\n",
      "Epoch 22/100\n",
      "60/60 [==============================] - 14s 236ms/step - loss: 1.4657 - acc: 0.6791\n",
      "Epoch 23/100\n",
      "60/60 [==============================] - 14s 236ms/step - loss: 1.5420 - acc: 0.6450\n",
      "Epoch 24/100\n",
      "60/60 [==============================] - 15s 243ms/step - loss: 1.4524 - acc: 0.7050\n",
      "Epoch 25/100\n",
      "60/60 [==============================] - 14s 229ms/step - loss: 1.5139 - acc: 0.6750\n",
      "Epoch 26/100\n",
      "60/60 [==============================] - 14s 240ms/step - loss: 1.3780 - acc: 0.7133\n",
      "Epoch 27/100\n",
      "60/60 [==============================] - 14s 232ms/step - loss: 1.4670 - acc: 0.7011\n",
      "Epoch 28/100\n",
      "60/60 [==============================] - 14s 231ms/step - loss: 1.3932 - acc: 0.6900\n",
      "Epoch 29/100\n",
      "60/60 [==============================] - 15s 243ms/step - loss: 1.4271 - acc: 0.6833\n",
      "Epoch 30/100\n",
      "60/60 [==============================] - 14s 240ms/step - loss: 1.5113 - acc: 0.6717\n",
      "Epoch 31/100\n",
      "60/60 [==============================] - 14s 232ms/step - loss: 1.4789 - acc: 0.6750\n",
      "Epoch 32/100\n",
      "60/60 [==============================] - 14s 234ms/step - loss: 1.4812 - acc: 0.6857\n",
      "Epoch 33/100\n",
      "60/60 [==============================] - 14s 233ms/step - loss: 1.3673 - acc: 0.7100\n",
      "Epoch 34/100\n",
      "60/60 [==============================] - 14s 234ms/step - loss: 1.4070 - acc: 0.6867\n",
      "Epoch 35/100\n",
      "60/60 [==============================] - 14s 235ms/step - loss: 1.3685 - acc: 0.7167\n",
      "Epoch 36/100\n",
      "60/60 [==============================] - 14s 237ms/step - loss: 1.4087 - acc: 0.7183\n",
      "Epoch 37/100\n",
      "60/60 [==============================] - 14s 235ms/step - loss: 1.3739 - acc: 0.7050\n",
      "Epoch 38/100\n",
      "60/60 [==============================] - 14s 238ms/step - loss: 1.2535 - acc: 0.7246\n",
      "Epoch 39/100\n",
      "60/60 [==============================] - 14s 235ms/step - loss: 1.3864 - acc: 0.7250\n",
      "Epoch 40/100\n",
      "60/60 [==============================] - 14s 236ms/step - loss: 1.3675 - acc: 0.6917\n",
      "Epoch 41/100\n",
      "60/60 [==============================] - 14s 240ms/step - loss: 1.3134 - acc: 0.7117\n",
      "Epoch 42/100\n",
      "60/60 [==============================] - 14s 235ms/step - loss: 1.3196 - acc: 0.7283\n",
      "Epoch 43/100\n",
      "60/60 [==============================] - 14s 235ms/step - loss: 1.4055 - acc: 0.6896\n",
      "Epoch 44/100\n",
      "60/60 [==============================] - 15s 243ms/step - loss: 1.3407 - acc: 0.7200\n",
      "Epoch 45/100\n",
      "60/60 [==============================] - 14s 238ms/step - loss: 1.1875 - acc: 0.7567\n",
      "Epoch 46/100\n",
      "60/60 [==============================] - 14s 233ms/step - loss: 1.3250 - acc: 0.7217\n",
      "Epoch 47/100\n",
      "60/60 [==============================] - 14s 236ms/step - loss: 1.3643 - acc: 0.7117\n",
      "Epoch 48/100\n",
      "60/60 [==============================] - 14s 230ms/step - loss: 1.3413 - acc: 0.7174\n",
      "Epoch 49/100\n",
      "60/60 [==============================] - 14s 234ms/step - loss: 1.1871 - acc: 0.7567\n",
      "Epoch 50/100\n",
      "60/60 [==============================] - 14s 234ms/step - loss: 1.3540 - acc: 0.7350\n",
      "Epoch 51/100\n",
      "60/60 [==============================] - 14s 235ms/step - loss: 1.3877 - acc: 0.6983\n",
      "Epoch 52/100\n",
      "60/60 [==============================] - 14s 233ms/step - loss: 1.2876 - acc: 0.7167\n",
      "Epoch 53/100\n",
      "60/60 [==============================] - 15s 246ms/step - loss: 1.2642 - acc: 0.7183\n",
      "Epoch 54/100\n",
      "60/60 [==============================] - 14s 234ms/step - loss: 1.2846 - acc: 0.7233\n",
      "Epoch 55/100\n",
      "60/60 [==============================] - 15s 242ms/step - loss: 1.3978 - acc: 0.6967\n",
      "Epoch 56/100\n",
      "60/60 [==============================] - 14s 233ms/step - loss: 1.2759 - acc: 0.7250\n",
      "Epoch 57/100\n",
      "60/60 [==============================] - 14s 238ms/step - loss: 1.2710 - acc: 0.7300\n",
      "Epoch 58/100\n",
      "60/60 [==============================] - 14s 236ms/step - loss: 1.2351 - acc: 0.7583\n",
      "Epoch 59/100\n",
      "60/60 [==============================] - 14s 236ms/step - loss: 1.3262 - acc: 0.7278\n",
      "Epoch 60/100\n",
      "60/60 [==============================] - 15s 242ms/step - loss: 1.2340 - acc: 0.7483\n",
      "Epoch 61/100\n",
      "60/60 [==============================] - 14s 238ms/step - loss: 1.2791 - acc: 0.7383\n",
      "Epoch 62/100\n",
      "60/60 [==============================] - 14s 233ms/step - loss: 1.1907 - acc: 0.7283\n",
      "Epoch 63/100\n",
      "60/60 [==============================] - 14s 236ms/step - loss: 1.2937 - acc: 0.7317\n",
      "Epoch 64/100\n",
      "60/60 [==============================] - 14s 238ms/step - loss: 1.2750 - acc: 0.7350\n",
      "Epoch 65/100\n",
      "60/60 [==============================] - 14s 239ms/step - loss: 1.1701 - acc: 0.7400\n",
      "Epoch 66/100\n",
      "60/60 [==============================] - 14s 234ms/step - loss: 1.2197 - acc: 0.7450\n",
      "Epoch 67/100\n",
      "60/60 [==============================] - 14s 234ms/step - loss: 1.1886 - acc: 0.7717\n",
      "Epoch 68/100\n",
      "60/60 [==============================] - 14s 240ms/step - loss: 1.2896 - acc: 0.7450\n",
      "Epoch 69/100\n",
      "60/60 [==============================] - 14s 234ms/step - loss: 1.2301 - acc: 0.7400\n",
      "Epoch 70/100\n",
      "60/60 [==============================] - 15s 246ms/step - loss: 1.1607 - acc: 0.7557\n",
      "Epoch 71/100\n",
      "60/60 [==============================] - 14s 240ms/step - loss: 1.2307 - acc: 0.7467\n",
      "Epoch 72/100\n",
      "60/60 [==============================] - 14s 236ms/step - loss: 1.3145 - acc: 0.7433\n",
      "Epoch 73/100\n",
      "60/60 [==============================] - 14s 233ms/step - loss: 1.1939 - acc: 0.7417\n",
      "Epoch 74/100\n",
      "60/60 [==============================] - 14s 238ms/step - loss: 1.1556 - acc: 0.7400\n",
      "Epoch 75/100\n",
      "60/60 [==============================] - 14s 237ms/step - loss: 1.2508 - acc: 0.7515\n",
      "Epoch 76/100\n",
      "60/60 [==============================] - 15s 245ms/step - loss: 1.1608 - acc: 0.7650\n",
      "Epoch 77/100\n",
      "60/60 [==============================] - 14s 235ms/step - loss: 1.2486 - acc: 0.7367\n",
      "Epoch 78/100\n",
      "60/60 [==============================] - 14s 232ms/step - loss: 1.2416 - acc: 0.7333\n",
      "Epoch 79/100\n",
      "60/60 [==============================] - 14s 240ms/step - loss: 1.2072 - acc: 0.7500\n",
      "Epoch 80/100\n",
      "60/60 [==============================] - 14s 232ms/step - loss: 1.0900 - acc: 0.7665\n",
      "Epoch 81/100\n",
      "60/60 [==============================] - 14s 237ms/step - loss: 1.1056 - acc: 0.7900\n",
      "Epoch 82/100\n",
      "60/60 [==============================] - 14s 236ms/step - loss: 1.0318 - acc: 0.7733\n",
      "Epoch 83/100\n",
      "60/60 [==============================] - 14s 236ms/step - loss: 1.2215 - acc: 0.7400\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 15s 243ms/step - loss: 1.2017 - acc: 0.7550\n",
      "Epoch 85/100\n",
      "60/60 [==============================] - 14s 235ms/step - loss: 1.1901 - acc: 0.7583\n",
      "Epoch 86/100\n",
      "60/60 [==============================] - 14s 242ms/step - loss: 1.2432 - acc: 0.7680\n",
      "Epoch 87/100\n",
      "60/60 [==============================] - 14s 235ms/step - loss: 1.0789 - acc: 0.7783\n",
      "Epoch 88/100\n",
      "60/60 [==============================] - 14s 233ms/step - loss: 1.1343 - acc: 0.7533\n",
      "Epoch 89/100\n",
      "60/60 [==============================] - 14s 237ms/step - loss: 1.1871 - acc: 0.7283\n",
      "Epoch 90/100\n",
      "60/60 [==============================] - 14s 236ms/step - loss: 1.1959 - acc: 0.7533\n",
      "Epoch 91/100\n",
      "60/60 [==============================] - 14s 236ms/step - loss: 1.0550 - acc: 0.7698\n",
      "Epoch 92/100\n",
      "60/60 [==============================] - 14s 239ms/step - loss: 1.2344 - acc: 0.7417\n",
      "Epoch 93/100\n",
      "60/60 [==============================] - 14s 234ms/step - loss: 1.1473 - acc: 0.7600\n",
      "Epoch 94/100\n",
      "60/60 [==============================] - 14s 233ms/step - loss: 1.1155 - acc: 0.7950\n",
      "Epoch 95/100\n",
      "60/60 [==============================] - 15s 245ms/step - loss: 1.1398 - acc: 0.7600\n",
      "Epoch 96/100\n",
      "60/60 [==============================] - 14s 234ms/step - loss: 1.1417 - acc: 0.7513\n",
      "Epoch 97/100\n",
      "60/60 [==============================] - 14s 234ms/step - loss: 1.1507 - acc: 0.7417\n",
      "Epoch 98/100\n",
      "60/60 [==============================] - 14s 233ms/step - loss: 1.0852 - acc: 0.7700\n",
      "Epoch 99/100\n",
      "60/60 [==============================] - 14s 241ms/step - loss: 1.1221 - acc: 0.7850\n",
      "Epoch 100/100\n",
      "60/60 [==============================] - 14s 239ms/step - loss: 1.1394 - acc: 0.7817\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "train_generator,\n",
    "steps_per_epoch = 60,\n",
    "epochs = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('VGG19_acc.jpeg')\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.savefig(\"VGG19_loss.jpeg\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 12, 12, 512)       20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 73728)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               37749248  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 48,337,991\n",
      "Trainable params: 37,752,839\n",
      "Non-trainable params: 10,585,152\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\danny han\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "pretrained_conv_base.trainable = True\n",
    "trainable = False\n",
    "for layer in pretrained_conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        trainable = True\n",
    "    if trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        \n",
    "        layer.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60/60 [==============================] - 20s 334ms/step - loss: 1.3663 - acc: 0.5630 - val_loss: 1.4373 - val_acc: 0.5855\n",
      "Epoch 2/100\n",
      "60/60 [==============================] - 16s 274ms/step - loss: 1.3176 - acc: 0.6150 - val_loss: 1.2581 - val_acc: 0.6480\n",
      "Epoch 3/100\n",
      "60/60 [==============================] - 17s 280ms/step - loss: 1.2248 - acc: 0.6217 - val_loss: 1.2187 - val_acc: 0.6781\n",
      "Epoch 4/100\n",
      "60/60 [==============================] - 17s 275ms/step - loss: 1.2466 - acc: 0.6296 - val_loss: 1.3134 - val_acc: 0.6439\n",
      "Epoch 5/100\n",
      "60/60 [==============================] - 17s 276ms/step - loss: 1.1105 - acc: 0.6667 - val_loss: 1.2963 - val_acc: 0.6320\n",
      "Epoch 6/100\n",
      "60/60 [==============================] - 17s 280ms/step - loss: 1.1764 - acc: 0.6600 - val_loss: 1.3912 - val_acc: 0.6439\n",
      "Epoch 7/100\n",
      "60/60 [==============================] - 16s 271ms/step - loss: 1.1157 - acc: 0.6815 - val_loss: 1.2634 - val_acc: 0.6559\n",
      "Epoch 8/100\n",
      "60/60 [==============================] - 16s 275ms/step - loss: 1.1514 - acc: 0.6567 - val_loss: 1.2421 - val_acc: 0.6801\n",
      "Epoch 9/100\n",
      "60/60 [==============================] - 17s 281ms/step - loss: 1.0875 - acc: 0.6767 - val_loss: 1.2929 - val_acc: 0.6320\n",
      "Epoch 10/100\n",
      "60/60 [==============================] - 17s 279ms/step - loss: 1.0819 - acc: 0.6798 - val_loss: 1.3541 - val_acc: 0.6237\n",
      "Epoch 11/100\n",
      "60/60 [==============================] - 17s 276ms/step - loss: 1.0626 - acc: 0.7250 - val_loss: 1.2420 - val_acc: 0.6781\n",
      "Epoch 12/100\n",
      "60/60 [==============================] - 16s 274ms/step - loss: 1.0483 - acc: 0.6917 - val_loss: 1.2785 - val_acc: 0.6940\n",
      "Epoch 13/100\n",
      "60/60 [==============================] - 16s 273ms/step - loss: 1.0297 - acc: 0.7228 - val_loss: 1.1912 - val_acc: 0.7042\n",
      "Epoch 14/100\n",
      "60/60 [==============================] - 17s 275ms/step - loss: 1.0015 - acc: 0.7117 - val_loss: 1.2930 - val_acc: 0.7022\n",
      "Epoch 15/100\n",
      "60/60 [==============================] - 17s 279ms/step - loss: 1.0181 - acc: 0.7367 - val_loss: 1.3511 - val_acc: 0.6420\n",
      "Epoch 16/100\n",
      "60/60 [==============================] - 17s 277ms/step - loss: 1.0197 - acc: 0.7300 - val_loss: 1.2519 - val_acc: 0.7082\n",
      "Epoch 17/100\n",
      "60/60 [==============================] - 17s 277ms/step - loss: 0.9836 - acc: 0.7167 - val_loss: 1.3008 - val_acc: 0.7082\n",
      "Epoch 18/100\n",
      "60/60 [==============================] - 16s 272ms/step - loss: 0.9157 - acc: 0.7700 - val_loss: 1.3193 - val_acc: 0.6540\n",
      "Epoch 19/100\n",
      "60/60 [==============================] - 16s 268ms/step - loss: 0.9614 - acc: 0.7324 - val_loss: 1.1612 - val_acc: 0.6982\n",
      "Epoch 20/100\n",
      "60/60 [==============================] - 17s 285ms/step - loss: 0.9640 - acc: 0.7167 - val_loss: 1.2507 - val_acc: 0.6841\n",
      "Epoch 21/100\n",
      "60/60 [==============================] - 17s 284ms/step - loss: 0.9351 - acc: 0.7483 - val_loss: 1.2890 - val_acc: 0.6800\n",
      "Epoch 22/100\n",
      "60/60 [==============================] - 16s 270ms/step - loss: 0.9344 - acc: 0.7215 - val_loss: 1.2423 - val_acc: 0.6660\n",
      "Epoch 23/100\n",
      "60/60 [==============================] - 17s 275ms/step - loss: 0.8941 - acc: 0.7700 - val_loss: 1.2618 - val_acc: 0.7243\n",
      "Epoch 24/100\n",
      "60/60 [==============================] - 16s 272ms/step - loss: 0.9607 - acc: 0.7367 - val_loss: 1.2669 - val_acc: 0.7020\n",
      "Epoch 25/100\n",
      "60/60 [==============================] - 16s 269ms/step - loss: 0.8786 - acc: 0.7613 - val_loss: 1.2210 - val_acc: 0.7183\n",
      "Epoch 26/100\n",
      "60/60 [==============================] - 17s 275ms/step - loss: 0.8520 - acc: 0.7867 - val_loss: 1.1622 - val_acc: 0.7485\n",
      "Epoch 27/100\n",
      "60/60 [==============================] - 17s 283ms/step - loss: 0.9554 - acc: 0.7400 - val_loss: 1.3036 - val_acc: 0.6761\n",
      "Epoch 28/100\n",
      "60/60 [==============================] - 17s 275ms/step - loss: 0.8976 - acc: 0.7593 - val_loss: 1.1611 - val_acc: 0.7360\n",
      "Epoch 29/100\n",
      "60/60 [==============================] - 17s 278ms/step - loss: 0.8793 - acc: 0.7683 - val_loss: 1.3959 - val_acc: 0.6801\n",
      "Epoch 30/100\n",
      "60/60 [==============================] - 16s 268ms/step - loss: 0.8865 - acc: 0.7667 - val_loss: 1.1800 - val_acc: 0.7243\n",
      "Epoch 31/100\n",
      "60/60 [==============================] - 16s 267ms/step - loss: 0.9076 - acc: 0.7543 - val_loss: 1.4127 - val_acc: 0.6580\n",
      "Epoch 32/100\n",
      "60/60 [==============================] - 16s 269ms/step - loss: 0.8361 - acc: 0.7883 - val_loss: 1.1719 - val_acc: 0.6962\n",
      "Epoch 33/100\n",
      "60/60 [==============================] - 16s 267ms/step - loss: 0.8051 - acc: 0.7950 - val_loss: 1.5314 - val_acc: 0.6398\n",
      "Epoch 34/100\n",
      "60/60 [==============================] - 16s 269ms/step - loss: 0.8461 - acc: 0.7896 - val_loss: 1.2545 - val_acc: 0.7080\n",
      "Epoch 35/100\n",
      "60/60 [==============================] - 16s 274ms/step - loss: 0.8334 - acc: 0.7767 - val_loss: 1.2298 - val_acc: 0.7344\n",
      "Epoch 36/100\n",
      "60/60 [==============================] - 16s 267ms/step - loss: 0.7965 - acc: 0.8000 - val_loss: 1.2510 - val_acc: 0.6761\n",
      "Epoch 37/100\n",
      "60/60 [==============================] - 17s 277ms/step - loss: 0.7467 - acc: 0.8128 - val_loss: 1.2280 - val_acc: 0.7400\n",
      "Epoch 38/100\n",
      "60/60 [==============================] - 16s 265ms/step - loss: 0.7508 - acc: 0.8267 - val_loss: 1.2217 - val_acc: 0.7223\n",
      "Epoch 39/100\n",
      "60/60 [==============================] - 16s 268ms/step - loss: 0.8544 - acc: 0.7750 - val_loss: 1.4009 - val_acc: 0.6559\n",
      "Epoch 40/100\n",
      "60/60 [==============================] - 16s 270ms/step - loss: 0.7662 - acc: 0.7948 - val_loss: 1.1632 - val_acc: 0.7540\n",
      "Epoch 41/100\n",
      "60/60 [==============================] - 16s 274ms/step - loss: 0.7540 - acc: 0.8083 - val_loss: 1.4551 - val_acc: 0.6579\n",
      "Epoch 42/100\n",
      "60/60 [==============================] - 17s 282ms/step - loss: 0.7565 - acc: 0.8100 - val_loss: 1.2196 - val_acc: 0.7304\n",
      "Epoch 43/100\n",
      "60/60 [==============================] - 16s 272ms/step - loss: 0.6917 - acc: 0.8576 - val_loss: 1.3323 - val_acc: 0.6922\n",
      "Epoch 44/100\n",
      "60/60 [==============================] - 16s 267ms/step - loss: 0.7202 - acc: 0.8250 - val_loss: 1.5259 - val_acc: 0.6460\n",
      "Epoch 45/100\n",
      "60/60 [==============================] - 17s 277ms/step - loss: 0.7610 - acc: 0.8133 - val_loss: 1.1892 - val_acc: 0.7183\n",
      "Epoch 46/100\n",
      "60/60 [==============================] - 17s 276ms/step - loss: 0.7223 - acc: 0.8265 - val_loss: 1.4277 - val_acc: 0.6922\n",
      "Epoch 47/100\n",
      "60/60 [==============================] - 17s 275ms/step - loss: 0.7147 - acc: 0.8133 - val_loss: 1.3231 - val_acc: 0.7140\n",
      "Epoch 48/100\n",
      "60/60 [==============================] - 17s 277ms/step - loss: 0.7579 - acc: 0.8200 - val_loss: 1.4143 - val_acc: 0.6720\n",
      "Epoch 49/100\n",
      "60/60 [==============================] - 16s 272ms/step - loss: 0.6933 - acc: 0.8280 - val_loss: 1.2888 - val_acc: 0.7364\n",
      "Epoch 50/100\n",
      "60/60 [==============================] - 16s 274ms/step - loss: 0.6797 - acc: 0.8250 - val_loss: 1.3825 - val_acc: 0.7140\n",
      "Epoch 51/100\n",
      "60/60 [==============================] - 17s 287ms/step - loss: 0.7170 - acc: 0.8033 - val_loss: 1.4995 - val_acc: 0.6640\n",
      "Epoch 52/100\n",
      "60/60 [==============================] - 17s 280ms/step - loss: 0.6975 - acc: 0.8409 - val_loss: 1.2336 - val_acc: 0.7666\n",
      "Epoch 53/100\n",
      "60/60 [==============================] - 17s 281ms/step - loss: 0.7250 - acc: 0.8133 - val_loss: 1.3380 - val_acc: 0.7080\n",
      "Epoch 54/100\n",
      "60/60 [==============================] - 17s 280ms/step - loss: 0.7166 - acc: 0.8200 - val_loss: 1.4102 - val_acc: 0.6680\n",
      "Epoch 55/100\n",
      "60/60 [==============================] - 16s 275ms/step - loss: 0.7050 - acc: 0.8033 - val_loss: 1.2933 - val_acc: 0.7284\n",
      "Epoch 56/100\n",
      "60/60 [==============================] - 17s 275ms/step - loss: 0.6169 - acc: 0.8517 - val_loss: 1.4859 - val_acc: 0.6840\n",
      "Epoch 57/100\n",
      "60/60 [==============================] - 17s 286ms/step - loss: 0.7038 - acc: 0.8300 - val_loss: 1.4360 - val_acc: 0.6841\n",
      "Epoch 58/100\n",
      "60/60 [==============================] - 16s 272ms/step - loss: 0.6515 - acc: 0.8581 - val_loss: 1.2398 - val_acc: 0.7082\n",
      "Epoch 59/100\n",
      "60/60 [==============================] - 17s 275ms/step - loss: 0.6504 - acc: 0.8350 - val_loss: 1.7484 - val_acc: 0.6820\n",
      "Epoch 60/100\n",
      "60/60 [==============================] - 17s 281ms/step - loss: 0.6502 - acc: 0.8367 - val_loss: 1.3868 - val_acc: 0.6942\n",
      "Epoch 61/100\n",
      "60/60 [==============================] - 17s 278ms/step - loss: 0.7174 - acc: 0.8194 - val_loss: 1.3711 - val_acc: 0.6700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "60/60 [==============================] - 17s 277ms/step - loss: 0.6287 - acc: 0.8600 - val_loss: 1.3249 - val_acc: 0.7364\n",
      "Epoch 63/100\n",
      "60/60 [==============================] - 16s 273ms/step - loss: 0.6539 - acc: 0.8500 - val_loss: 1.4402 - val_acc: 0.6840\n",
      "Epoch 64/100\n",
      "60/60 [==============================] - 16s 274ms/step - loss: 0.6425 - acc: 0.8331 - val_loss: 1.3469 - val_acc: 0.7103\n",
      "Epoch 65/100\n",
      "60/60 [==============================] - 16s 271ms/step - loss: 0.6427 - acc: 0.8417 - val_loss: 1.3658 - val_acc: 0.7304\n",
      "Epoch 66/100\n",
      "60/60 [==============================] - 17s 282ms/step - loss: 0.6544 - acc: 0.8317 - val_loss: 1.3835 - val_acc: 0.7160\n",
      "Epoch 67/100\n",
      "60/60 [==============================] - 17s 279ms/step - loss: 0.6203 - acc: 0.8433 - val_loss: 1.3466 - val_acc: 0.7163\n",
      "Epoch 68/100\n",
      "60/60 [==============================] - 17s 279ms/step - loss: 0.5990 - acc: 0.8650 - val_loss: 1.3601 - val_acc: 0.6922\n",
      "Epoch 69/100\n",
      "60/60 [==============================] - 16s 274ms/step - loss: 0.5765 - acc: 0.8661 - val_loss: 1.3688 - val_acc: 0.7000\n",
      "Epoch 70/100\n",
      "60/60 [==============================] - 17s 278ms/step - loss: 0.5627 - acc: 0.8750 - val_loss: 1.8081 - val_acc: 0.6761\n",
      "Epoch 71/100\n",
      "60/60 [==============================] - 17s 282ms/step - loss: 0.6144 - acc: 0.8433 - val_loss: 1.4074 - val_acc: 0.7364\n",
      "Epoch 72/100\n",
      "60/60 [==============================] - 17s 278ms/step - loss: 0.6292 - acc: 0.8531 - val_loss: 1.2603 - val_acc: 0.7320\n",
      "Epoch 73/100\n",
      "60/60 [==============================] - 16s 275ms/step - loss: 0.5757 - acc: 0.8583 - val_loss: 1.6008 - val_acc: 0.6861\n",
      "Epoch 74/100\n",
      "60/60 [==============================] - 17s 281ms/step - loss: 0.5709 - acc: 0.8767 - val_loss: 1.3265 - val_acc: 0.6962\n",
      "Epoch 75/100\n",
      "60/60 [==============================] - 16s 271ms/step - loss: 0.6393 - acc: 0.8278 - val_loss: 1.3820 - val_acc: 0.7000\n",
      "Epoch 76/100\n",
      "60/60 [==============================] - 16s 275ms/step - loss: 0.5785 - acc: 0.8650 - val_loss: 1.3654 - val_acc: 0.7123\n",
      "Epoch 77/100\n",
      "60/60 [==============================] - 16s 270ms/step - loss: 0.5816 - acc: 0.8800 - val_loss: 1.4168 - val_acc: 0.7022\n",
      "Epoch 78/100\n",
      "60/60 [==============================] - 17s 276ms/step - loss: 0.5963 - acc: 0.8594 - val_loss: 1.3368 - val_acc: 0.7140\n",
      "Epoch 79/100\n",
      "60/60 [==============================] - 17s 279ms/step - loss: 0.5859 - acc: 0.8550 - val_loss: 1.3679 - val_acc: 0.7203\n",
      "Epoch 80/100\n",
      "60/60 [==============================] - 16s 274ms/step - loss: 0.5310 - acc: 0.8917 - val_loss: 1.7744 - val_acc: 0.6720\n",
      "Epoch 81/100\n",
      "60/60 [==============================] - 16s 273ms/step - loss: 0.5509 - acc: 0.8696 - val_loss: 1.5328 - val_acc: 0.7304\n",
      "Epoch 82/100\n",
      "60/60 [==============================] - 16s 273ms/step - loss: 0.5396 - acc: 0.8817 - val_loss: 1.4825 - val_acc: 0.7100\n",
      "Epoch 83/100\n",
      "60/60 [==============================] - 17s 284ms/step - loss: 0.5190 - acc: 0.8867 - val_loss: 1.5589 - val_acc: 0.6881\n",
      "Epoch 84/100\n",
      "60/60 [==============================] - 17s 277ms/step - loss: 0.5461 - acc: 0.8765 - val_loss: 1.3551 - val_acc: 0.7203\n",
      "Epoch 85/100\n",
      "60/60 [==============================] - 16s 272ms/step - loss: 0.5489 - acc: 0.8783 - val_loss: 1.3989 - val_acc: 0.7040\n",
      "Epoch 86/100\n",
      "60/60 [==============================] - 16s 269ms/step - loss: 0.5355 - acc: 0.8967 - val_loss: 1.5811 - val_acc: 0.6901\n",
      "Epoch 87/100\n",
      "60/60 [==============================] - 16s 270ms/step - loss: 0.5240 - acc: 0.8765 - val_loss: 1.3961 - val_acc: 0.7264\n",
      "Epoch 88/100\n",
      "60/60 [==============================] - 16s 275ms/step - loss: 0.5574 - acc: 0.8667 - val_loss: 1.4703 - val_acc: 0.7300\n",
      "Epoch 89/100\n",
      "60/60 [==============================] - 17s 276ms/step - loss: 0.5378 - acc: 0.8667 - val_loss: 1.3841 - val_acc: 0.7384\n",
      "Epoch 90/100\n",
      "60/60 [==============================] - 16s 270ms/step - loss: 0.4975 - acc: 0.8931 - val_loss: 1.5360 - val_acc: 0.6982\n",
      "Epoch 91/100\n",
      "60/60 [==============================] - 17s 277ms/step - loss: 0.4917 - acc: 0.8883 - val_loss: 1.6513 - val_acc: 0.6820\n",
      "Epoch 92/100\n",
      "60/60 [==============================] - 16s 272ms/step - loss: 0.5056 - acc: 0.8900 - val_loss: 1.5715 - val_acc: 0.7042\n",
      "Epoch 93/100\n",
      "60/60 [==============================] - 17s 284ms/step - loss: 0.5187 - acc: 0.8817 - val_loss: 1.4232 - val_acc: 0.7022\n",
      "Epoch 94/100\n",
      "60/60 [==============================] - 16s 274ms/step - loss: 0.5063 - acc: 0.8800 - val_loss: 1.3835 - val_acc: 0.7200\n",
      "Epoch 95/100\n",
      "60/60 [==============================] - 17s 277ms/step - loss: 0.5358 - acc: 0.8833 - val_loss: 1.7661 - val_acc: 0.6620\n",
      "Epoch 96/100\n",
      "60/60 [==============================] - 17s 276ms/step - loss: 0.5000 - acc: 0.8800 - val_loss: 1.5630 - val_acc: 0.6962\n",
      "Epoch 97/100\n",
      "60/60 [==============================] - 17s 279ms/step - loss: 0.5047 - acc: 0.8850 - val_loss: 1.4716 - val_acc: 0.7060\n",
      "Epoch 98/100\n",
      "60/60 [==============================] - 16s 272ms/step - loss: 0.4552 - acc: 0.9033 - val_loss: 1.6091 - val_acc: 0.6861\n",
      "Epoch 99/100\n",
      "60/60 [==============================] - 16s 274ms/step - loss: 0.5027 - acc: 0.8965 - val_loss: 1.6265 - val_acc: 0.6821\n",
      "Epoch 100/100\n",
      "60/60 [==============================] - 16s 273ms/step - loss: 0.4588 - acc: 0.9067 - val_loss: 1.4208 - val_acc: 0.7002\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "train_generator,\n",
    "steps_per_epoch=60,\n",
    "epochs=100,\n",
    "validation_data=validation_generator,\n",
    "validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "input_shape=(400, 400, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Xception_Trial_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 400, 400, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 199, 199, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 199, 199, 32) 128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 199, 199, 32) 0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 197, 197, 64) 18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 197, 197, 64) 256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 197, 197, 64) 0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 197, 197, 128 8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 197, 197, 128 512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 197, 197, 128 0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 197, 197, 128 17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 197, 197, 128 512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 99, 99, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 99, 99, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 99, 99, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 99, 99, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 99, 99, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 99, 99, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 99, 99, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 99, 99, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 99, 99, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 99, 99, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 50, 50, 256)  32768       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 50, 50, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 50, 50, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 50, 50, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 50, 50, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 50, 50, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 50, 50, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 50, 50, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 50, 50, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 50, 50, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 25, 25, 728)  186368      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 25, 25, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 25, 25, 728)  2912        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 25, 25, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 25, 25, 728)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 25, 25, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 25, 25, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 25, 25, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 25, 25, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 25, 25, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 25, 25, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 25, 25, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 25, 25, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 25, 25, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 25, 25, 728)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 25, 25, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 25, 25, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 25, 25, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 25, 25, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 25, 25, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 25, 25, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 25, 25, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 25, 25, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 25, 25, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 25, 25, 728)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 25, 25, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 25, 25, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 25, 25, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 25, 25, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 25, 25, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 25, 25, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 25, 25, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 25, 25, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 25, 25, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 25, 25, 728)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 25, 25, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 25, 25, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 25, 25, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 25, 25, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 25, 25, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 25, 25, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 25, 25, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 25, 25, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 25, 25, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 25, 25, 728)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 25, 25, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 25, 25, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 25, 25, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 25, 25, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 25, 25, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 25, 25, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 25, 25, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 25, 25, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 25, 25, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 25, 25, 728)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 25, 25, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 25, 25, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 25, 25, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 25, 25, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 25, 25, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 25, 25, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 25, 25, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 25, 25, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 25, 25, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 25, 25, 728)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 25, 25, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 25, 25, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 25, 25, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 25, 25, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 25, 25, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 25, 25, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 25, 25, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 25, 25, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 25, 25, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 25, 25, 728)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 25, 25, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 25, 25, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 25, 25, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 25, 25, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 25, 25, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 25, 25, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 25, 25, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 25, 25, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 25, 25, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 25, 25, 728)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 25, 25, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 25, 25, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 25, 25, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 25, 25, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 25, 25, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 13, 13, 1024) 745472      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 13, 13, 1024) 0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 13, 13, 1024) 4096        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 13, 13, 1024) 0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 13, 13, 1536) 1582080     add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 13, 13, 1536) 6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 13, 13, 1536) 0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 13, 13, 2048) 3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 13, 13, 2048) 8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 13, 13, 2048) 0           block14_sepconv2_bn[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 20,861,480\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,861,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
